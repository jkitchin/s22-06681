#+title: Data science and machine learning in chemical engineering

This class is about data, models and data analysis in chemical engineering. We will cover topics including

- Reading data from different sources (urls, files, etc)
- Visualization of data
- Working with multidimensional data
- Using Pandas
- Using scikit-learn for machine learning

The class will utilize Python and Jupyter notebooks. I am going to assume you have some basic fluency in Python, and Jupyter notebooks, e.g. you have at least taken 06-623 (Mathematical modeling of chemical engineering processes). If you have not had that course, you will want to review the lectures at https://github.com/jkitchin/f19-06623.

* First things first

The [[https://canvas.cmu.edu/courses/26911/assignments/syllabus][syllabus]].

* Getting started

We will be using Deepnote for most lectures and assignments.



* Getting started with Deepnote

Let's review some simple markup:

#+BEGIN_EXAMPLE
# heading
## subheading

**bold**

*italics*

Separate text by blank lines so they appear as "paragraphs"

[a link](https://github.com)

1. a
2. numbered list
  1. you can also have sub-items


- a bullet list
  - with sub-bullets
#+END_EXAMPLE

You will be using both code and Markdown cells to document what you are doing, why you are doing it, and what it means.

Jupyter notebooks are a way to /communicate/ your work. The are also /executable/ documents. Some important points to remember:

1. The code is for a computer, and it represents your attempt to convert some idea in your head into an executable program.
2. You cannot expect others to read your mind and guess what your intentions were. You have to use the narrative text in the Markdown cells to explain what you are trying to do, what approximations you have to make to accomplish it.

Let's see a real example of this. Don't worry about what this code means yet, it is an idea I want to get across.

What is happening here?

#+BEGIN_SRC ipython
import numpy as np

x = np.linspace(0, np.pi)
y = np.sin(x)
0.5* ((x[1:] - x[:-1]) * (y[1:] + y[:-1])).sum()
#+END_SRC

#+RESULTS:
:results:
: 1.9993148493240622
:end:

It is hard to tell. I know (for now) what is in my head, but even future me will have trouble figuring out what this means.

Compare that to:

I want to compute the integral $y = \int_0^\pi sin(x) dx$. I will approximate the integral using the trapezoid rule, and vectorize it as described at https://berkeley-stat159-f17.github.io/stat159-f17/lectures/09-intro-numpy/trapezoid..html

#+BEGIN_SRC ipython
import numpy as np

x = np.linspace(0, np.pi)
y = np.sin(x)
0.5* ( (x[1:] - x[:-1]) * (y[1:] + y[:-1])).sum()
#+END_SRC

#+RESULTS:
:results:
: 1.9993148493240622
:end:

Next we compare the method to the =np.trapz= library function.

#+BEGIN_SRC ipython
np.trapz(y, x)
#+END_SRC

#+RESULTS:
:results:
: 1.9993148493240622
:end:

This is one of the most critical points in data science. The data itself is not meaningful if you don't know what it was obtained for, and your analysis may not be useful if nobody can tell what you were trying to do. It is critical that you provide documentation to guide people reviewing your work (including future you).

* Break

Please get up, stretch, etc. We will be back in two minutes.

* Data

We will start with some high level thinking about what we mean by data, why we get it, and what we do with it.

Data are things we measure, assume to be facts, and that we use to learn about the process the data was collected from. It is usually a set of numerical values that are collected. It is critical to know something about your data so you understand what analysis may be appropriate. Data is a plural word. Datum is the singular form of data.

For example, here are two sets of data on my weight:

1. [7.5, 46, 150, 157]
2. [156, 155, 158, 157]


We are missing some context on these. The first set is data over four decades, while the second set is over four days. It doesn't really make sense to average the first set, whereas the average of the second set gives you a good idea of how my weight fluctuates on a daily basis.

Data by itself is not helpful. It is analysis of data that is helpful, but you have to know what the data is supposed to represent to know if the analysis is helpful.

There are many kinds of analysis one can do: statistical, regression, integration, etc. Each of these has the purpose of extracting information from the data.

Let's consider the average and standard deviation of the second weights above. To perform this analysis, we need a computational tool, we will use Python. We will extensively use numpy arrays for data analysis. We start by making an array in a variable called weights. Then, we simply call the mean and std functions of that array inside a formatted string.

#+BEGIN_SRC ipython
import numpy as np

weights = np.array([156, 155, 158, 157])
print(f'My average weight is {np.mean(weights)} \pm {np.std(weights):1.1f} lbs.')
#+END_SRC

#+RESULTS:
:results:
My average weight is 156.5 \pm 1.1 lbs.

:end:

This analysis makes sense /if/ we think my weight fluctuates about some average with a normal distribution of fluctuations. We do not have enough data to determine if it is normal here, but it is worth noting that assumption underlies the analysis. Note, we also assume that each measurement is independent, and uncorrelated with the previous and next measurement. If I weigh myself only once a day, that is probably reasonable. If these are sequential weights separated by 1 minute, then either something is wrong with the scale or, I am doing something funny in how I weigh myself.

What factors could affect the weight measurement?
1. What am I wearing?
2. What and when did I last eat/drink?
3. When was the last time I exercised and for how long?
4. Are all the measurements from the same scale?

The answers for all these constitute the /metadata/, which is data about the data. If we had access to this metadata, we might ask if any of these factors influence the measurements. As we consider more dimensions like this,  it becomes inconvenient to visualize and build models with conventional tools, and we then will turn to machine learning.

There is a lot to learn about using data before we get to machine learning though.



* Reference material

- https://jupyterlab.readthedocs.io/en/stable/user/interface.html
  - There is a lot here that some of you may find interesting.

- Keyboard shortcuts https://blog.ja-ke.tech/assets/jupyterlab-shortcuts/Shortcuts.png
  - You do not need to memorize these, but they will eventually help you do some things faster

- https://docs.scipy.org/doc/numpy/reference/routines.statistics.html
  - Familiarize yourself with what is possible. Do not try to memorize all these. The main point is be familiar so that you can better judge in the future if there is likely to be a library function you can use, or if you need to implement a function yourself.

* Reading material

Please start reading at  https://jakevdp.github.io/PythonDataScienceHandbook/02.02-the-basics-of-numpy-arrays.html and read through Chapter 2 to the end https://jakevdp.github.io/PythonDataScienceHandbook/02.09-structured-data-numpy.html. We will cover some of this material next week. Note that you can open these in "Colab" from the web site to work on the examples interactively.


